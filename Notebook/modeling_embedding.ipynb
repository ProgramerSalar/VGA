{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Optional, Union\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import math \n",
    "\n",
    "from diffusers.models.activations import get_activation\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Get 1D sincos position embeding for grid\n",
    "\n",
    "\n",
    "- generate a 1-dimensional sinusoidal positional embedding for a given list of position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, \n",
    "                                      pos):\n",
    "\n",
    "    \"\"\" \n",
    "    embed_dim: output dimension for each position \n",
    "    pos: a list of positions to be encoded: size (M, ) \n",
    "    output: (M, D)\n",
    "    \"\"\"\n",
    "\n",
    "    # This function checks if `embed_dim` is divisible by 2. if not it raises a ValueError.\n",
    "    if embed_dim % 2 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 2\")\n",
    "    \n",
    "\n",
    "\n",
    "    # `omega` is an array of size (embed_dim // 2). It is scaled by dividing by `embed_dim / 2.0` and then transformed using the formula  ( \\omega = \\frac{1}{10000^{\\omega}} )\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.0 \n",
    "    omega = 1.0 / 10000**omega    # (D/2,)\n",
    "\n",
    "\n",
    "    # The `pos` array is reshaped to ensure it is a 1D array of size (M,)\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum(\"m,d-> md\", pos, omega)    # The function uses `np.einsum` to compute the outer product of `pos` and `omega`, resulting in an array `output` of size (M, D/2)\n",
    "\n",
    "\n",
    "    # `emb_sin` and `emb_cos` are computed by applying the sine and cosine function to `out`, respectively.\n",
    "    emb_sin = np.sin(out)\n",
    "    emb_cos = np.cos(out)\n",
    "\n",
    "    # The sine and cosine embeddings are concatenated along the last axis to form the final embedding of size (M, D)\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    \n",
    "    return emb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Get 1d sin cos position embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed(\n",
    "        embed_dim,\n",
    "        num_frames,\n",
    "        cls_token=False,\n",
    "        extra_tokens=0\n",
    "        ): \n",
    "    \n",
    "\n",
    "    t = np.arange(num_frames, dtype=np.float32)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim=embed_dim, \n",
    "                                                  pos=t)   # (T, D)\n",
    "    \n",
    "\n",
    "    if cls_token and extra_tokens > 0:\n",
    "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    return pos_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 GEt 2d sin cos embedding from grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_2d_sincos_pos_embed_from_grid(embed_dim,\n",
    "                                       grid):\n",
    "    \n",
    "    if embed_dim % 2 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 2\")\n",
    "    \n",
    "\n",
    "\n",
    "    # use half of dimension to encode grid_h \n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])   # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])   # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis = 1)  # (H*W, D)\n",
    "    return emb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 GEt 2D sin cos position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(\n",
    "        embed_dim,\n",
    "        grid_size,\n",
    "        cls_token=False,\n",
    "        extra_tokens = 0,\n",
    "        interpolation_scale = 1.0,\n",
    "        base_size = 16\n",
    "): \n",
    "    \n",
    "    \"\"\" \n",
    "    grid_size: int of the grid height and width return: pos_embed: [grid_size * grid_size, embed_dim] or \n",
    "    [1 + grid_size * grid_size, embed_dim] (w / or w/o cls_token)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(grid_size, int):\n",
    "        grid_size = (grid_size, grid_size)\n",
    "\n",
    "\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32) / (grid_size[0] / base_size) / interpolation_scale\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32) / (grid_size[1] / base_size) / interpolation_scale\n",
    "\n",
    "    grid = np.meshgrid(grid_w, grid_h)   # here w goes first \n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size[1], grid_size[0]])\n",
    "    pos_embed =  get_2d_sincos_pos_embed_from_grid(embed_dim=embed_dim,\n",
    "                                                   grid=grid)\n",
    "    \n",
    "    if cls_token and extra_tokens > 0:\n",
    "        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Get Timestep Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(\n",
    "        timesteps: torch.Tensor,\n",
    "        embedding_dim: int,\n",
    "        flip_sin_to_cos: bool = False,\n",
    "        downscale_freq_shift: float = 1,\n",
    "        scale: float = 1,\n",
    "        max_period: int = 10000\n",
    "): \n",
    "    \n",
    "\n",
    "    \"\"\" \n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embedings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
    "    :param embedding_dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n",
    "\n",
    "    half_dim = embedding_dim // 2 \n",
    "    exponent = -math.log(max_period) * torch.arange(\n",
    "        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n",
    "    )\n",
    "\n",
    "    exponent = exponent / (half_dim - downscale_freq_shift)\n",
    "\n",
    "    emb = torch.exp(exponent)\n",
    "    emb = timesteps[:, None].float() * emb[None, :]\n",
    "\n",
    "    # scale embeddings \n",
    "    emb = scale * emb \n",
    "\n",
    "    # concate sine and cosine embeddings \n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "    # flip sine and cosine embeddings \n",
    "    if flip_sin_to_cos:\n",
    "        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)\n",
    "\n",
    "\n",
    "    # zero pad \n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "\n",
    "    return emb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timesteps(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_channels: int,\n",
    "                 flip_sin_to_cos: bool,\n",
    "                 downscale_freq_shift: float\n",
    "                 ): \n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.flip_sin_to_cos = flip_sin_to_cos\n",
    "        self.downscale_freq_shift = downscale_freq_shift\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        t_emb = get_timestep_embedding(\n",
    "            timesteps,\n",
    "            self.num_channels,\n",
    "            flip_sin_to_cos=self.flip_sin_to_cos,\n",
    "            downscale_freq_shift=self.downscale_freq_shift\n",
    "        )\n",
    "\n",
    "        return t_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7. Timestep Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            time_embed_dim: int,\n",
    "            act_fn: str = \"silu\",\n",
    "            out_dim: int = None,\n",
    "            post_act_fn: Optional[str] = None,\n",
    "            sample_proj_bias = True\n",
    "    ): \n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_channels, time_embed_dim, sample_proj_bias)\n",
    "        self.act = get_activation(act_fn)\n",
    "        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim, sample_proj_bias)\n",
    "\n",
    "\n",
    "    def forward(self, sample):\n",
    "        sample = self.linear_1(sample)\n",
    "        sample = self.act(sample)\n",
    "        sample = self.linear_2(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.8 Text Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProjection(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features, \n",
    "                 hidden_size,\n",
    "                 act_fn=\"silu\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features=in_features, out_features=hidden_size, bias=True)\n",
    "        self.act_1 = get_activation(act_fn)\n",
    "        self.linear_2 = nn.Linear(in_features=hidden_size, out_features=hidden_size, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, caption):\n",
    "        hidden_states = self.linear_1(caption)\n",
    "        hidden_states = self.act_1(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.9 Combined Timestep Condition Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineTimestepConditionEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 pooled_projection_dim\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.time_proj = Timesteps(num_channels=256,\n",
    "                                   flip_sin_to_cos=True,\n",
    "                                   downscale_freq_shift=0)\n",
    "        \n",
    "        self.timestep_embedder = TimestepEmbedding(in_channels=256,\n",
    "                                                   time_embed_dim=embedding_dim)\n",
    "        \n",
    "        self.text_embedder = TextProjection(in_features=pooled_projection_dim,\n",
    "                                            hidden_size=embedding_dim,\n",
    "                                            act_fn=\"silu\")\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                timestep,\n",
    "                pooled_projection):\n",
    "        \n",
    "\n",
    "        timesteps_proj = self.time_proj(timestep)\n",
    "        timestep_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))   # (N, D)\n",
    "        pooled_projection = self.text_embedder(pooled_projection)\n",
    "        conditioning = timestep_emb + pooled_projection \n",
    "        \n",
    "        return conditioning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.10 Combined Time Step Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTimestepEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim):\n",
    "        super().__init__()\n",
    "        self.time_proj = Timesteps(num_channels=256,\n",
    "                                   flip_sin_to_cos=True,\n",
    "                                   downscale_freq_shift=0)\n",
    "        \n",
    "        self.timestep_embedder = TimestepEmbedding(in_channels=256,\n",
    "                                                   time_embed_dim=embedding_dim)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, timestep):\n",
    "        timesteps_proj = self.time_proj(timestep)\n",
    "        timesteps_emb = self.timestep_embedder(timesteps_proj)     # (N, D)\n",
    "\n",
    "        return timesteps_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.11 Patch Embedding 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed3D(nn.Module):\n",
    "\n",
    "    \"\"\"Support the 3D Tensor input\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "            \n",
    "            self,\n",
    "            height=128,\n",
    "            width = 128,\n",
    "            patch_size = 2,\n",
    "            in_channels = 16,\n",
    "            embed_dim = 1536,\n",
    "            layer_norm = False,\n",
    "            bias=True,\n",
    "            interpolation_scale = 1,\n",
    "            pos_embed_type = \"sincos\",\n",
    "            temp_pos_embed_type = \"rope\",\n",
    "            pos_embed_max_size = 192,\n",
    "            max_num_frames = 64,\n",
    "            add_temp_pos_embed = False,\n",
    "            interp_condition_pos = False \n",
    "\n",
    "    ): \n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = (height // patch_size) * (width // patch_size)\n",
    "        self.layer_norm = layer_norm\n",
    "        self.pos_embed_max_size = pos_embed_max_size\n",
    "\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim,\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=patch_size,\n",
    "            bias=bias\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        if layer_norm:\n",
    "            self.norm = nn.LayerNorm(embed_dim, \n",
    "                                     elementwise_affine=False,\n",
    "                                     eps=1e-6)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            self.norm = None \n",
    "\n",
    "\n",
    "\n",
    "        self.path_size = patch_size\n",
    "        self.height, self.width = height // patch_size, width // patch_size\n",
    "        self.base_size = height // patch_size\n",
    "        self.interpolation_scale = interpolation_scale\n",
    "        self.add_temp_pos_embed = add_temp_pos_embed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # calculation positional embeddings based on max size or default \n",
    "        if pos_embed_max_size:\n",
    "            grid_size = pos_embed_max_size\n",
    "\n",
    "        else: \n",
    "            grid_size = int(num_patches**0.5)\n",
    "\n",
    "\n",
    "\n",
    "        if pos_embed_type is None:\n",
    "            self.pos_embed = None \n",
    "\n",
    "\n",
    "\n",
    "        elif pos_embed_type == \"sincos\":\n",
    "            \n",
    "            pos_embed = get_2d_sincos_pos_embed(\n",
    "                embed_dim=embed_dim,\n",
    "                grid_size=grid_size,\n",
    "                base_size=self.base_size,\n",
    "                interpolation_scale=self.interpolation_scale\n",
    "            )\n",
    "\n",
    "            persistant = True if pos_embed_max_size else False \n",
    "            self.register_buffer(\"pos_embed\", \n",
    "                                 torch.from_numpy(pos_embed).float().unsqueeze(0),\n",
    "                                  persistent=persistant )\n",
    "            \n",
    "\n",
    "\n",
    "            if add_temp_pos_embed and temp_pos_embed_type == \"sincos\":\n",
    "                \n",
    "                time_pos_embed = get_1d_sincos_pos_embed(embed_dim=embed_dim,\n",
    "                                                         num_frames=max_num_frames)\n",
    "                \n",
    "                self.register_buffer(\"temp_pos_embed\",\n",
    "                                     torch.from_numpy(time_pos_embed).float().unsqueeze(0),\n",
    "                                     persistent=True)\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "        elif pos_embed_type == \"rope\":\n",
    "            print(\"Using the rotary position embedding\")\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pos_embed_type: {pos_embed_type}\")\n",
    "        \n",
    "\n",
    "        self.pos_embed_type = pos_embed_type\n",
    "        self.temp_pos_embed_type = temp_pos_embed_type\n",
    "        self.interp_condition_pos = interp_condition_pos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.12 Cropped position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropped_pos_embed(self,\n",
    "                      height,\n",
    "                      width,\n",
    "                      ori_height,\n",
    "                      ori_width):\n",
    "    \n",
    "\n",
    "    \"\"\" \n",
    "    Crops positional embeddings for SD3 compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    if self.pos_embed_max_size is None:\n",
    "        raise ValueError(\" `pos_embed_max_size` must be set for cropping. \")\n",
    "    \n",
    "\n",
    "    height = height // self.patch_size \n",
    "    width = width // self.patch_size \n",
    "    ori_height = ori_height // self.patch_size \n",
    "    ori_width = ori_width // self.patch_size \n",
    "\n",
    "\n",
    "    assert ori_height >= height, \"The ori_heigth needs >= height\"\n",
    "    assert ori_width >= width, \"The ori_width needs >= width\"\n",
    "\n",
    "\n",
    "    if height > self.pos_embed_max_size:\n",
    "        raise ValueError(\n",
    "            f\"Height ({height}) cannot be greater than 'pos_embed_max_size': {self.pos_embed_max_size}.\"\n",
    "\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "    if width > self.pos_embed_max_size:\n",
    "        raise ValueError(\n",
    "            f\"Width ({width}) cannot be greater than `pos_embed_max_size`: {self.pos_embed_max_size}.\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    if self.interp_condition_pos:\n",
    "        top = (self.pos_embed_max_size - ori_height) // 2 \n",
    "        left = (self.pos_embed_max_size - ori_width) // 2 \n",
    "\n",
    "        spatial_pos_embed = self.pos_embed.reshape(1, \n",
    "                                                   self.pos_embed_max_size,\n",
    "                                                   self.pos_embed_max_size,\n",
    "                                                   -1)\n",
    "        \n",
    "\n",
    "        spatial_pos_embed = spatial_pos_embed[:, \n",
    "                                              top: top + height, \n",
    "                                              left: left + width, :]\n",
    "        \n",
    "\n",
    "\n",
    "    spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])\n",
    "\n",
    "    return spatial_pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.13 Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(self,\n",
    "                 latent,\n",
    "                 time_index = 0,\n",
    "                 ori_height = None,\n",
    "                 ori_width = None):\n",
    "    \n",
    "    if self.pos_embed_max_size is not None:\n",
    "        height, width = latent.shape[-2:]\n",
    "\n",
    "    \n",
    "    else:\n",
    "        height, width = latent.shape[-2] // self.patch_size, latent.shape[-1] // self.patch_size \n",
    "\n",
    "\n",
    "    bs = latent.shape[0]\n",
    "    temp = latent.shape[2]\n",
    "\n",
    "    latent = rearrange(latent, 'b c t h w -> (b t) c h w')\n",
    "    latent = self.proj(latent)\n",
    "    latent = latent.flatten(2).transpose(1, 2)  # (BT)CHW -> (BT)NC\n",
    "\n",
    "\n",
    "    if self.layer_norm:\n",
    "        latent = self.norm(latent)\n",
    "\n",
    "\n",
    "\n",
    "    if self.pos_embed_type == 'sincos':\n",
    "\n",
    "        # Spatial position embedding, Interpolate or crop positional embeddings as needed \n",
    "        if self.pos_embed_max_size:\n",
    "            pos_embed = self.cropped_pos_embed(height, width, ori_height, ori_width)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not implemented sincos pos embed without sd3 max pos crop\")        \n",
    "            if self.height != height or self.width != width:\n",
    "                pos_embed = get_2d_sincos_pos_embed(\n",
    "                    embed_dim=self.pos_embed.shape[-1],\n",
    "                    grid_size=(height, width),\n",
    "                    base_size=self.base_size,\n",
    "                    interpolation_scale=self.interpolation_scale\n",
    "\n",
    "                )\n",
    "                pos_embed = torch.from_numpy(pos_embed).float().unsqueeze(0).to(latent.device)\n",
    "\n",
    "\n",
    "            else:\n",
    "                pos_embed = self.pos_embed \n",
    "\n",
    "\n",
    "\n",
    "        if self.add_temp_pos_embed and self.temp_pos_embed_type == \"sincos\":\n",
    "\n",
    "            latent_dtype = latent.dtype \n",
    "            latent = latent + pos_embed\n",
    "            latent = rearrange(latent, '(b t) n c -> (b n) t c', t = temp)\n",
    "            latent = latent + self.temp_pos_embed[:,\n",
    "                                                   time_index: time_index,\n",
    "                                                   temp, :\n",
    "                                                  ]\n",
    "            \n",
    "\n",
    "            latent = latent.to(latent_dtype)\n",
    "            latent = rearrange(latent, '(b t) n c -> b t n c', b=bs, t=temp)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            latent = (latent + pos_embed).to(latent.dtype)\n",
    "            latent = rearrange(latent, '(b t) n c -> b t n c', b = bs, t=temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        assert self.pos_embed_type == \"rope\", \"Only supporting the sincos and rope embedding\"\n",
    "        latent = rearrange(latent, '(b t) n c -> b t n c', b=bs, t=temp)\n",
    "\n",
    "\n",
    "    return latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, latent):\n",
    "\n",
    "    \"\"\" \n",
    "    Arguments: \n",
    "        past_condition_latents (Torch.FloatTensor): The past latent during the generation \n",
    "        flatten_input (bool): True indicate flatten the latent into 1D sequence\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(latent, list):\n",
    "        output_list = []\n",
    "\n",
    "\n",
    "        for latent_ in latent:\n",
    "            if not isinstance(latent_, list):\n",
    "                latent_ = [latent_]\n",
    "\n",
    "\n",
    "            output_latent = []\n",
    "            time_index = 0 \n",
    "\n",
    "            ori_height, ori_width = latent_[-1].shape[-2:]\n",
    "\n",
    "            for each_latent in latent_:\n",
    "                hidden_state = self.forward_func(each_latent,\n",
    "                                                 time_index=time_index,\n",
    "                                                 ori_height=ori_height,\n",
    "                                                 ori_width=ori_width\n",
    "                                                 )\n",
    "                \n",
    "                time_index += each_latent.shape[2]\n",
    "                hidden_state = rearrange(hidden_state, \"b t n c -> b (t n) c\")\n",
    "                output_latent.append(hidden_state)\n",
    "\n",
    "\n",
    "            output_latent = torch.cat(output_latent, dim=1)\n",
    "            output_list.append(output_latent)\n",
    "\n",
    "\n",
    "\n",
    "        return output_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        hidden_state = self.forward_func(latent)\n",
    "        hidden_state = rearrange(hidden_state, \"b t n c -> b (t n) c\")\n",
    "\n",
    "        return hidden_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
